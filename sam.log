2025-09-06 14:01:52,512 - solace_ai_connector.common.log_init - INFO - Logging configured via INI file 'configs/logging_config.ini' (specified by LOGGING_CONFIG_PATH) from solace_ai_connector.common.log module import.
2025-09-06 14:01:52,544 - a2a.utils.telemetry - DEBUG - Start tracing for a2a.utils.helpers.create_task_obj, is_async_func False
2025-09-06 14:01:52,544 - a2a.utils.telemetry - DEBUG - Start tracing for a2a.utils.helpers.append_artifact_to_task, is_async_func False
2025-09-06 14:01:54,838 - solace_ai_connector.common.log_init - INFO - Logging configured via INI file 'configs/logging_config.ini' (specified by LOGGING_CONFIG_PATH) from solace_ai_connector.common.log module import.
2025-09-06 14:01:54,869 - a2a.utils.telemetry - DEBUG - Start tracing for a2a.utils.helpers.create_task_obj, is_async_func False
2025-09-06 14:01:54,870 - a2a.utils.telemetry - DEBUG - Start tracing for a2a.utils.helpers.append_artifact_to_task, is_async_func False
2025-09-06 14:01:55,187 - config_portal.backend.server - INFO - Starting Flask app on 127.0.0.1:5002
2025-09-06 14:03:16,520 - LiteLLM - DEBUG - 

2025-09-06 14:03:16,521 - LiteLLM - DEBUG - [92mRequest to litellm:[0m
2025-09-06 14:03:16,521 - LiteLLM - DEBUG - [92mlitellm.completion(model='openai/gemini-2.0-flash-001', api_key='AIzaSyACBEMOHxQtP48h3iOcuQV8qSISn06t6BY', base_url='https://generativelanguage.googleapis.com/v1beta/openai', messages=[{'role': 'user', 'content': 'Say OK'}])[0m
2025-09-06 14:03:16,521 - LiteLLM - DEBUG - 

2025-09-06 14:03:16,522 - LiteLLM - DEBUG - self.optional_params: {}
2025-09-06 14:03:16,522 - LiteLLM - DEBUG - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False
2025-09-06 14:03:16,530 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-2.0-flash-001; provider = openai
2025-09-06 14:03:16,530 - LiteLLM - DEBUG - 
LiteLLM: Params passed to completion() {'model': 'gemini-2.0-flash-001', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'openai', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': 'Say OK'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None}
2025-09-06 14:03:16,530 - LiteLLM - DEBUG - 
LiteLLM: Non-Default params passed to completion() {}
2025-09-06 14:03:16,530 - LiteLLM - DEBUG - Final returned optional params: {'extra_body': {}}
2025-09-06 14:03:16,530 - LiteLLM - DEBUG - self.optional_params: {'extra_body': {}}
2025-09-06 14:03:16,530 - LiteLLM - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-001', 'combined_model_name': 'openai/gemini-2.0-flash-001', 'stripped_model_name': 'gemini-2.0-flash-001', 'combined_stripped_model_name': 'openai/gemini-2.0-flash-001', 'custom_llm_provider': 'openai'}
2025-09-06 14:03:16,531 - LiteLLM - DEBUG - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json
2025-09-06 14:03:16,531 - LiteLLM - DEBUG - Error getting model info: This model isn't mapped yet. model=gemini-2.0-flash-001, custom_llm_provider=openai. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.
2025-09-06 14:03:16,554 - LiteLLM - DEBUG - [92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/openai/ \
-d '{'model': 'gemini-2.0-flash-001', 'messages': [{'role': 'user', 'content': 'Say OK'}], 'extra_body': {}}'
[0m

2025-09-06 14:03:17,574 - LiteLLM - DEBUG - RAW RESPONSE:
{"id": "ZXe8aKjhBbCm_uMPltyT6A8", "choices": [{"finish_reason": "stop", "index": 0, "logprobs": null, "message": {"content": "OK\n", "refusal": null, "role": "assistant", "annotations": null, "audio": null, "function_call": null, "tool_calls": null}}], "created": 1757181797, "model": "gemini-2.0-flash-001", "object": "chat.completion", "service_tier": null, "system_fingerprint": null, "usage": {"completion_tokens": 2, "prompt_tokens": 2, "total_tokens": 4, "completion_tokens_details": null, "prompt_tokens_details": null}}


2025-09-06 14:03:17,575 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-09-06 14:03:17,575 - LiteLLM - DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
2025-09-06 14:03:17,575 - LiteLLM - DEBUG - selected model name for cost calculation: openai/gemini-2.0-flash-001
2025-09-06 14:03:17,575 - LiteLLM - DEBUG - selected model name for cost calculation: openai/gemini-2.0-flash-001
2025-09-06 14:03:17,575 - LiteLLM - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-001', 'combined_model_name': 'openai/gemini-2.0-flash-001', 'stripped_model_name': 'gemini-2.0-flash-001', 'combined_stripped_model_name': 'openai/gemini-2.0-flash-001', 'custom_llm_provider': 'openai'}
2025-09-06 14:03:17,576 - LiteLLM - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-001', 'combined_model_name': 'openai/gemini-2.0-flash-001', 'stripped_model_name': 'gemini-2.0-flash-001', 'combined_stripped_model_name': 'openai/gemini-2.0-flash-001', 'custom_llm_provider': 'openai'}
2025-09-06 14:03:17,576 - LiteLLM - DEBUG - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json
2025-09-06 14:03:17,576 - LiteLLM - DEBUG - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json
2025-09-06 14:03:17,576 - LiteLLM - DEBUG - litellm.cost_calculator.py::completion_cost() - Error calculating cost for model=openai/gemini-2.0-flash-001 - This model isn't mapped yet. model=gemini-2.0-flash-001, custom_llm_provider=openai. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.
2025-09-06 14:03:17,576 - LiteLLM - DEBUG - litellm.cost_calculator.py::completion_cost() - Error calculating cost for model=openai/gemini-2.0-flash-001 - This model isn't mapped yet. model=gemini-2.0-flash-001, custom_llm_provider=openai. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.
2025-09-06 14:03:17,576 - LiteLLM - DEBUG - selected model name for cost calculation: openai/gemini-2.0-flash-001
2025-09-06 14:03:17,576 - LiteLLM - DEBUG - selected model name for cost calculation: gemini-2.0-flash-001
2025-09-06 14:03:17,576 - LiteLLM - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-001', 'combined_model_name': 'openai/gemini-2.0-flash-001', 'stripped_model_name': 'gemini-2.0-flash-001', 'combined_stripped_model_name': 'openai/gemini-2.0-flash-001', 'custom_llm_provider': 'openai'}
2025-09-06 14:03:17,576 - LiteLLM - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-001', 'combined_model_name': 'openai/gemini-2.0-flash-001', 'stripped_model_name': 'gemini-2.0-flash-001', 'combined_stripped_model_name': 'openai/gemini-2.0-flash-001', 'custom_llm_provider': 'openai'}
2025-09-06 14:03:17,576 - LiteLLM - DEBUG - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json
2025-09-06 14:03:17,576 - LiteLLM - DEBUG - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json
2025-09-06 14:03:17,576 - LiteLLM - DEBUG - litellm.cost_calculator.py::completion_cost() - Error calculating cost for model=openai/gemini-2.0-flash-001 - This model isn't mapped yet. model=gemini-2.0-flash-001, custom_llm_provider=openai. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.
2025-09-06 14:03:17,576 - LiteLLM - DEBUG - litellm.cost_calculator.py::completion_cost() - Error calculating cost for model=gemini-2.0-flash-001 - This model isn't mapped yet. model=gemini-2.0-flash-001, custom_llm_provider=openai. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.
2025-09-06 14:03:17,583 - LiteLLM - DEBUG - response_cost_failure_debug_information: {'error_str': "This model isn't mapped yet. model=gemini-2.0-flash-001, custom_llm_provider=openai. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.", 'traceback_str': 'Traceback (most recent call last):\n  File "/Users/sertanavdan/miniconda3/envs/solace/lib/python3.13/site-packages/litellm/utils.py", line 4809, in _get_model_info_helper\n    raise ValueError(\n        "This model isn\'t mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json"\n    )\nValueError: This model isn\'t mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File "/Users/sertanavdan/miniconda3/envs/solace/lib/python3.13/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 1228, in _response_cost_calculator\n    response_cost = litellm.response_cost_calculator(\n        **response_cost_calculator_kwargs\n    )\n  File "/Users/sertanavdan/miniconda3/envs/solace/lib/python3.13/site-packages/litellm/cost_calculator.py", line 1037, in response_cost_calculator\n    raise e\n  File "/Users/sertanavdan/miniconda3/envs/solace/lib/python3.13/site-packages/litellm/cost_calculator.py", line 1021, in response_cost_calculator\n    response_cost = completion_cost(\n        completion_response=response_object,\n    ...<10 lines>...\n        litellm_logging_obj=litellm_logging_obj,\n    )\n  File "/Users/sertanavdan/miniconda3/envs/solace/lib/python3.13/site-packages/litellm/cost_calculator.py", line 935, in completion_cost\n    raise e\n  File "/Users/sertanavdan/miniconda3/envs/solace/lib/python3.13/site-packages/litellm/cost_calculator.py", line 928, in completion_cost\n    raise e\n  File "/Users/sertanavdan/miniconda3/envs/solace/lib/python3.13/site-packages/litellm/cost_calculator.py", line 890, in completion_cost\n    ) = cost_per_token(\n        ~~~~~~~~~~~~~~^\n        model=model,\n        ^^^^^^^^^^^^\n    ...<14 lines>...\n        rerank_billed_units=rerank_billed_units,\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File "/Users/sertanavdan/miniconda3/envs/solace/lib/python3.13/site-packages/litellm/cost_calculator.py", line 330, in cost_per_token\n    return openai_cost_per_token(model=model, usage=usage_block)\n  File "/Users/sertanavdan/miniconda3/envs/solace/lib/python3.13/site-packages/litellm/llms/openai/cost_calculation.py", line 33, in cost_per_token\n    return generic_cost_per_token(\n        model=model, usage=usage, custom_llm_provider="openai"\n    )\n  File "/Users/sertanavdan/miniconda3/envs/solace/lib/python3.13/site-packages/litellm/litellm_core_utils/llm_cost_calc/utils.py", line 233, in generic_cost_per_token\n    model_info = get_model_info(model=model, custom_llm_provider=custom_llm_provider)\n  File "/Users/sertanavdan/miniconda3/envs/solace/lib/python3.13/site-packages/litellm/utils.py", line 5020, in get_model_info\n    _model_info = _get_model_info_helper(\n        model=model,\n        custom_llm_provider=custom_llm_provider,\n    )\n  File "/Users/sertanavdan/miniconda3/envs/solace/lib/python3.13/site-packages/litellm/utils.py", line 4938, in _get_model_info_helper\n    raise Exception(\n    ...<3 lines>...\n    )\nException: This model isn\'t mapped yet. model=gemini-2.0-flash-001, custom_llm_provider=openai. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.\n', 'model': 'openai/gemini-2.0-flash-001', 'cache_hit': False, 'custom_llm_provider': 'openai', 'base_model': None, 'call_type': 'completion', 'custom_pricing': False}
2025-09-06 14:03:17,584 - LiteLLM - DEBUG - response_cost_failure_debug_information: {'error_str': "This model isn't mapped yet. model=gemini-2.0-flash-001, custom_llm_provider=openai. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.", 'traceback_str': 'Traceback (most recent call last):\n  File "/Users/sertanavdan/miniconda3/envs/solace/lib/python3.13/site-packages/litellm/utils.py", line 4809, in _get_model_info_helper\n    raise ValueError(\n        "This model isn\'t mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json"\n    )\nValueError: This model isn\'t mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File "/Users/sertanavdan/miniconda3/envs/solace/lib/python3.13/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 1228, in _response_cost_calculator\n    response_cost = litellm.response_cost_calculator(\n        **response_cost_calculator_kwargs\n    )\n  File "/Users/sertanavdan/miniconda3/envs/solace/lib/python3.13/site-packages/litellm/cost_calculator.py", line 1037, in response_cost_calculator\n    raise e\n  File "/Users/sertanavdan/miniconda3/envs/solace/lib/python3.13/site-packages/litellm/cost_calculator.py", line 1021, in response_cost_calculator\n    response_cost = completion_cost(\n        completion_response=response_object,\n    ...<10 lines>...\n        litellm_logging_obj=litellm_logging_obj,\n    )\n  File "/Users/sertanavdan/miniconda3/envs/solace/lib/python3.13/site-packages/litellm/cost_calculator.py", line 935, in completion_cost\n    raise e\n  File "/Users/sertanavdan/miniconda3/envs/solace/lib/python3.13/site-packages/litellm/cost_calculator.py", line 928, in completion_cost\n    raise e\n  File "/Users/sertanavdan/miniconda3/envs/solace/lib/python3.13/site-packages/litellm/cost_calculator.py", line 890, in completion_cost\n    ) = cost_per_token(\n        ~~~~~~~~~~~~~~^\n        model=model,\n        ^^^^^^^^^^^^\n    ...<14 lines>...\n        rerank_billed_units=rerank_billed_units,\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File "/Users/sertanavdan/miniconda3/envs/solace/lib/python3.13/site-packages/litellm/cost_calculator.py", line 330, in cost_per_token\n    return openai_cost_per_token(model=model, usage=usage_block)\n  File "/Users/sertanavdan/miniconda3/envs/solace/lib/python3.13/site-packages/litellm/llms/openai/cost_calculation.py", line 33, in cost_per_token\n    return generic_cost_per_token(\n        model=model, usage=usage, custom_llm_provider="openai"\n    )\n  File "/Users/sertanavdan/miniconda3/envs/solace/lib/python3.13/site-packages/litellm/litellm_core_utils/llm_cost_calc/utils.py", line 233, in generic_cost_per_token\n    model_info = get_model_info(model=model, custom_llm_provider=custom_llm_provider)\n  File "/Users/sertanavdan/miniconda3/envs/solace/lib/python3.13/site-packages/litellm/utils.py", line 5020, in get_model_info\n    _model_info = _get_model_info_helper(\n        model=model,\n        custom_llm_provider=custom_llm_provider,\n    )\n  File "/Users/sertanavdan/miniconda3/envs/solace/lib/python3.13/site-packages/litellm/utils.py", line 4938, in _get_model_info_helper\n    raise Exception(\n    ...<3 lines>...\n    )\nException: This model isn\'t mapped yet. model=gemini-2.0-flash-001, custom_llm_provider=openai. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.\n', 'model': 'gemini-2.0-flash-001', 'cache_hit': None, 'custom_llm_provider': 'openai', 'base_model': None, 'call_type': 'completion', 'custom_pricing': False}
2025-09-06 14:03:17,585 - LiteLLM - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-001', 'combined_model_name': 'openai/gemini-2.0-flash-001', 'stripped_model_name': 'gemini-2.0-flash-001', 'combined_stripped_model_name': 'openai/gemini-2.0-flash-001', 'custom_llm_provider': 'openai'}
2025-09-06 14:03:17,585 - LiteLLM - DEBUG - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json
2025-09-06 14:03:17,585 - LiteLLM - DEBUG - Model=gemini-2.0-flash-001 is not mapped in model cost map. Defaulting to None model_cost_information for standard_logging_payload
2025-09-06 14:03:17,585 - LiteLLM - DEBUG - Logging Details LiteLLM-Success Call streaming complete
2025-09-06 14:03:17,585 - LiteLLM - DEBUG - selected model name for cost calculation: openai/gemini-2.0-flash-001
2025-09-06 14:03:17,585 - LiteLLM - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-001', 'combined_model_name': 'openai/gemini-2.0-flash-001', 'stripped_model_name': 'gemini-2.0-flash-001', 'combined_stripped_model_name': 'openai/gemini-2.0-flash-001', 'custom_llm_provider': 'openai'}
2025-09-06 14:03:17,585 - LiteLLM - DEBUG - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json
2025-09-06 14:03:17,585 - LiteLLM - DEBUG - litellm.cost_calculator.py::completion_cost() - Error calculating cost for model=openai/gemini-2.0-flash-001 - This model isn't mapped yet. model=gemini-2.0-flash-001, custom_llm_provider=openai. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.
2025-09-06 14:03:17,585 - LiteLLM - DEBUG - selected model name for cost calculation: gemini-2.0-flash-001
2025-09-06 14:03:17,585 - LiteLLM - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-001', 'combined_model_name': 'openai/gemini-2.0-flash-001', 'stripped_model_name': 'gemini-2.0-flash-001', 'combined_stripped_model_name': 'openai/gemini-2.0-flash-001', 'custom_llm_provider': 'openai'}
2025-09-06 14:03:17,585 - LiteLLM - DEBUG - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json
2025-09-06 14:03:17,585 - LiteLLM - DEBUG - litellm.cost_calculator.py::completion_cost() - Error calculating cost for model=gemini-2.0-flash-001 - This model isn't mapped yet. model=gemini-2.0-flash-001, custom_llm_provider=openai. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.
2025-09-06 14:03:17,585 - LiteLLM - DEBUG - response_cost_failure_debug_information: {'error_str': "This model isn't mapped yet. model=gemini-2.0-flash-001, custom_llm_provider=openai. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.", 'traceback_str': 'Traceback (most recent call last):\n  File "/Users/sertanavdan/miniconda3/envs/solace/lib/python3.13/site-packages/litellm/utils.py", line 4809, in _get_model_info_helper\n    raise ValueError(\n        "This model isn\'t mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json"\n    )\nValueError: This model isn\'t mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File "/Users/sertanavdan/miniconda3/envs/solace/lib/python3.13/site-packages/litellm/litellm_core_utils/litellm_logging.py", line 1228, in _response_cost_calculator\n    response_cost = litellm.response_cost_calculator(\n        **response_cost_calculator_kwargs\n    )\n  File "/Users/sertanavdan/miniconda3/envs/solace/lib/python3.13/site-packages/litellm/cost_calculator.py", line 1037, in response_cost_calculator\n    raise e\n  File "/Users/sertanavdan/miniconda3/envs/solace/lib/python3.13/site-packages/litellm/cost_calculator.py", line 1021, in response_cost_calculator\n    response_cost = completion_cost(\n        completion_response=response_object,\n    ...<10 lines>...\n        litellm_logging_obj=litellm_logging_obj,\n    )\n  File "/Users/sertanavdan/miniconda3/envs/solace/lib/python3.13/site-packages/litellm/cost_calculator.py", line 935, in completion_cost\n    raise e\n  File "/Users/sertanavdan/miniconda3/envs/solace/lib/python3.13/site-packages/litellm/cost_calculator.py", line 928, in completion_cost\n    raise e\n  File "/Users/sertanavdan/miniconda3/envs/solace/lib/python3.13/site-packages/litellm/cost_calculator.py", line 890, in completion_cost\n    ) = cost_per_token(\n        ~~~~~~~~~~~~~~^\n        model=model,\n        ^^^^^^^^^^^^\n    ...<14 lines>...\n        rerank_billed_units=rerank_billed_units,\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File "/Users/sertanavdan/miniconda3/envs/solace/lib/python3.13/site-packages/litellm/cost_calculator.py", line 330, in cost_per_token\n    return openai_cost_per_token(model=model, usage=usage_block)\n  File "/Users/sertanavdan/miniconda3/envs/solace/lib/python3.13/site-packages/litellm/llms/openai/cost_calculation.py", line 33, in cost_per_token\n    return generic_cost_per_token(\n        model=model, usage=usage, custom_llm_provider="openai"\n    )\n  File "/Users/sertanavdan/miniconda3/envs/solace/lib/python3.13/site-packages/litellm/litellm_core_utils/llm_cost_calc/utils.py", line 233, in generic_cost_per_token\n    model_info = get_model_info(model=model, custom_llm_provider=custom_llm_provider)\n  File "/Users/sertanavdan/miniconda3/envs/solace/lib/python3.13/site-packages/litellm/utils.py", line 5020, in get_model_info\n    _model_info = _get_model_info_helper(\n        model=model,\n        custom_llm_provider=custom_llm_provider,\n    )\n  File "/Users/sertanavdan/miniconda3/envs/solace/lib/python3.13/site-packages/litellm/utils.py", line 4938, in _get_model_info_helper\n    raise Exception(\n    ...<3 lines>...\n    )\nException: This model isn\'t mapped yet. model=gemini-2.0-flash-001, custom_llm_provider=openai. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.\n', 'model': 'gemini-2.0-flash-001', 'cache_hit': None, 'custom_llm_provider': 'openai', 'base_model': None, 'call_type': 'completion', 'custom_pricing': False}
2025-09-06 14:03:17,585 - LiteLLM - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-2.0-flash-001', 'combined_model_name': 'openai/gemini-2.0-flash-001', 'stripped_model_name': 'gemini-2.0-flash-001', 'combined_stripped_model_name': 'openai/gemini-2.0-flash-001', 'custom_llm_provider': 'openai'}
2025-09-06 14:03:17,585 - LiteLLM - DEBUG - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json
2025-09-06 14:03:17,585 - LiteLLM - DEBUG - Model=gemini-2.0-flash-001 is not mapped in model cost map. Defaulting to None model_cost_information for standard_logging_payload
